{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is, while using the same code structure as the notebook *cnn_classification*, to compare models pretrained on other datasets and that are well known by the community. In particular, we will test on the CIFAR10 dataset the following networks : VGG16, ResNet50 and DenseNet121. These networks have won some challenges and were state-of-the-art model during a certain time in image classification. We will see the one to fits best to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the CIFAR10 has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "cifar = load_dataset(\"cifar10\",)\n",
    "i = cifar[\"train\"][\"img\"][0]\n",
    "print(i.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract labels for classes for the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "labels = cifar[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "    \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before converting to Dataset object, we define some transformations (where data augmentation could be done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "     transforms.Resize((32,32)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset class is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    # Constructor \n",
    "    def __init__(self, X_data, Y_data, transform=transform):\n",
    "        self.len = len(X_data)\n",
    "        self.x = X_data\n",
    "        self.y = Y_data\n",
    "        self.transform = transform\n",
    "             \n",
    "    # Getter\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index] \n",
    "        y = self.y[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)     \n",
    "        return x, y\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the DataLoader instances that will be used in the training, validation and testing of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "\n",
    "dataset_val = Dataset( X_data = cifar[\"train\"][\"img\"][0:10000], Y_data = cifar[\"train\"][\"label\"][0:10000])\n",
    "val_loader = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=True)\n",
    "dataset_train = Dataset( X_data = cifar[\"train\"][\"img\"][10000:-1], Y_data = cifar[\"train\"][\"label\"][10000:-1])\n",
    "train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataset_test = Dataset( X_data = cifar[\"test\"][\"img\"], Y_data = cifar[\"test\"][\"label\"])\n",
    "test_loader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *torchvision* models interface to load those networks. More networks could have been added to the list. We ensure that the model has been pretrained and that just the final layers will have to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DL-project/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/DL-project/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/miniconda3/envs/DL-project/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/miniconda3/envs/DL-project/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_names = ['resnet50', 'vgg16', 'densenet121']\n",
    "models_to_compare = {}\n",
    "\n",
    "for name in model_names:\n",
    "    model = getattr(models, name)(pretrained=True)\n",
    "    models_to_compare[name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following function to make sure that the output is givenby  10 neurons corresponding to the 10 classes of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model(model, num_classes):\n",
    "    # Freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Modify the final layer based on model architecture\n",
    "    if isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif isinstance(model, models.VGG):\n",
    "        # For VGG models, classifier is a Sequential module\n",
    "        num_ftrs = model.classifier[0].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    elif isinstance(model, models.DenseNet):\n",
    "        # For DenseNet models, classifier is a Linear layer\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model architecture {type(model)} not supported\")\n",
    "\n",
    "    return model\n",
    "\n",
    "for name in models_to_compare:\n",
    "    models_to_compare[name] = modify_model(models_to_compare[name], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A metric is defined below and a function to get optimizer that run only on the last layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def get_optimizer(model):\n",
    "    # Only parameters of final layers are being optimized\n",
    "    return optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")     #Use GPU if avalaible\n",
    "    model.to(device)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    epoch_train_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_acc = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        ###### TRAINING #######\n",
    "        print(\"Training\")\n",
    "        model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        train_acc = []\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            acc = accuracy(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            train_losses.append(loss.item())\n",
    "            train_acc.append(acc.item())\n",
    "        \n",
    "        epoch_train_loss.append(sum(train_losses)/len(train_losses))\n",
    "        epoch_train_acc.append(sum(train_acc)/len(train_acc))\n",
    "\n",
    "        ### VALIDATION ###\n",
    "        print(\"Validation\")\n",
    "        val_losses = []\n",
    "        val_acc = []\n",
    "        val_loss_total = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_loader:\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "                acc = accuracy(output, y)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                val_acc.append(acc.item())\n",
    "                \n",
    "        epoch_val_loss.append(sum(val_losses)/len(val_losses))\n",
    "        epoch_val_acc.append(sum(val_acc)/len(val_acc))\n",
    "\n",
    "        print(f\"Valid epoch {epoch} loss:\", \"{:.4f}\".format(sum(val_losses)/len(val_losses)), \"acc\", \"{:.4f}\".format(sum(val_acc)/len(val_acc)),\"\\n\")\n",
    "        print()\n",
    "        val_loss_total = sum(val_losses)/len(val_losses)\n",
    "        if val_loss_total < best_loss:\n",
    "            best_loss = val_loss_total\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience = 3  \n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                break   \n",
    "            \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training function has been defined, we can call it on the different models and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training resnet50 model...\n",
      "Epoch 0/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 0 loss: 1.5826 acc 0.4654 \n",
      "\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 1 loss: 1.5396 acc 0.4714 \n",
      "\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 2 loss: 1.4832 acc 0.4981 \n",
      "\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 3 loss: 1.4787 acc 0.4972 \n",
      "\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 4 loss: 1.4515 acc 0.5051 \n",
      "\n",
      "\n",
      "\n",
      "Training vgg16 model...\n",
      "Epoch 0/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 0 loss: 1.1994 acc 0.5813 \n",
      "\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 1 loss: 1.1609 acc 0.5977 \n",
      "\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 2 loss: 1.1561 acc 0.6015 \n",
      "\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 3 loss: 1.1533 acc 0.6009 \n",
      "\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 4 loss: 1.1363 acc 0.6025 \n",
      "\n",
      "\n",
      "\n",
      "Training densenet121 model...\n",
      "Epoch 0/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 0 loss: 1.4708 acc 0.5069 \n",
      "\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 1 loss: 1.4221 acc 0.5185 \n",
      "\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 2 loss: 1.3793 acc 0.5289 \n",
      "\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 3 loss: 1.3599 acc 0.5342 \n",
      "\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "Training\n",
      "Validation\n",
      "Valid epoch 4 loss: 1.3574 acc 0.5369 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train each model\n",
    "trained_models = {}\n",
    "for name, model in models_to_compare.items():\n",
    "    print(f'\\nTraining {name} model...')\n",
    "    optimizer = get_optimizer(model)\n",
    "    trained_models[name] = train_model(model, criterion, optimizer, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            acc = accuracy(output, y)\n",
    "            \n",
    "            test_losses.append(loss.item())\n",
    "            test_acc.append(acc.item())\n",
    "\n",
    "    acc = sum(test_acc)/len(test_acc)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The oldest model (VGG16) seems to better perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model resnet50 Accuracy: 0.5161\n",
      "Model vgg16 Accuracy: 0.6095\n",
      "Model densenet121 Accuracy: 0.5320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate each model\n",
    "model_performances = {}\n",
    "for name, model in trained_models.items():\n",
    "    acc = evaluate_model(model)\n",
    "    model_performances[name] = acc\n",
    "    print(f'Model {name} Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  Accuracy\n",
      "0     resnet50  0.516113\n",
      "1        vgg16  0.609473\n",
      "2  densenet121  0.532031\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "performance_df = pd.DataFrame(list(model_performances.items()), columns=['Model', 'Accuracy'])\n",
    "print(performance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
