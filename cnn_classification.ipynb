{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar10 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "cifar = load_dataset(\"cifar10\",)\n",
    "i = cifar[\"train\"][\"img\"][0]\n",
    "print(i.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images in cifar10 has small size 32 x 32 which is faster for training. Moreover, the dataset has 10 classes and we can see that both train and test datasets are balanced with equal number of smaples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(cifar[\"train\"])\n",
    "df_train.groupby(by=\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(cifar[\"test\"])\n",
    "df_test.groupby(by=\"label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cifar[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "    \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaturalSceneClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4096,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "     transforms.Resize((32,32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "    # Constructor \n",
    "    def __init__(self, X_data, Y_data, transform=transform):\n",
    "        self.len = len(X_data)\n",
    "        self.x = X_data\n",
    "        self.y = Y_data\n",
    "        self.transform = transform\n",
    "             \n",
    "    # Getter\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index] \n",
    "        y = self.y[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)     \n",
    "        return x, y\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = pd.DataFrame(cifar[\"train\"][10000:-1])\n",
    "xx.groupby(\"label\").count()\n",
    "#yy = pd.DataFrame(cifar[\"train\"])\n",
    "#yy.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = Dataset( X_data = cifar[\"train\"][\"img\"][0:10000], Y_data = cifar[\"train\"][\"label\"][0:10000])\n",
    "val_loader = DataLoader(dataset=dataset_val, batch_size=128, shuffle=True)\n",
    "dataset_train = Dataset( X_data = cifar[\"train\"][\"img\"][10000:-1], Y_data = cifar[\"train\"][\"label\"][10000:-1])\n",
    "train_loader = DataLoader(dataset=dataset_train, batch_size=128, shuffle=True)\n",
    "dataset_test = Dataset( X_data = cifar[\"test\"][\"img\"], Y_data = cifar[\"test\"][\"label\"])\n",
    "test_loader = DataLoader(dataset=dataset_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview an image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label = dataset_train[110]\n",
    "\n",
    "def display_img(img,label):\n",
    "    print(f\"Label : {label}\")\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "#display the first image in the dataset\n",
    "display_img(img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the Optimizer and the Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaturalSceneClassification()\n",
    "model.to(\"cuda:0\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_new = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch loss 1.7866771556317997 acc 0.3274265144960568\n",
      "Val epoch loss 1.49209642712074 acc 0.4464003164556962\n",
      "Train epoch loss 1.333000196435581 acc 0.5129820066519057\n",
      "Val epoch loss 1.2590207797062547 acc 0.5355023734177216\n",
      "Train epoch loss 1.0888102648738094 acc 0.6044887704209397\n",
      "Val epoch loss 1.0342649425132364 acc 0.6237143987341772\n",
      "Train epoch loss 0.9271895883563227 acc 0.6679459690285948\n",
      "Val epoch loss 0.9539303364633005 acc 0.667128164556962\n",
      "Train epoch loss 0.7829874957712314 acc 0.7203807242381306\n",
      "Val epoch loss 0.9745298963558825 acc 0.6640625\n",
      "Train epoch loss 0.6716162053921733 acc 0.7640188397310032\n",
      "Val epoch loss 0.816970946290825 acc 0.7217167721518988\n",
      "Train epoch loss 0.5734268170766557 acc 0.7977882214247609\n",
      "Val epoch loss 0.8156805642043488 acc 0.7321993670886076\n",
      "Train epoch loss 0.4755494545062129 acc 0.832061558676223\n",
      "Val epoch loss 0.8282793496228471 acc 0.734375\n",
      "Train epoch loss 0.37939704159578197 acc 0.8645868990748835\n",
      "Val epoch loss 0.8606839828853365 acc 0.7354628164556962\n",
      "Train epoch loss 0.29158083432779525 acc 0.8956605938676828\n",
      "Val epoch loss 0.9792272659796702 acc 0.7396162974683544\n"
     ]
    }
   ],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    \n",
    "    model.train()\n",
    "    for x,y in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(\"cuda:0\")\n",
    "        y = y.to(\"cuda:0\")\n",
    "        output = model(x)\n",
    "        loss = loss_new(output, y)\n",
    "        acc = accuracy(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_acc.append(acc.item())\n",
    "        \n",
    "\n",
    "    print(\"Train epoch loss\", sum(train_losses)/len(train_losses), \"acc\", sum(train_acc)/len(train_acc))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_loader:\n",
    "\n",
    "            \n",
    "            x = x.to(\"cuda:0\")\n",
    "            y = y.to(\"cuda:0\")\n",
    "            output = model(x)\n",
    "            loss = loss_new(output, y)\n",
    "            acc = accuracy(output, y)\n",
    "            \n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            val_acc.append(acc.item())\n",
    "            \n",
    "\n",
    "    print(\"Val epoch loss\", sum(val_losses)/len(val_losses), \"acc\", sum(val_acc)/len(val_acc))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss 0.9665000317971918 acc 0.7389240506329114\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    \n",
    "    for x,y in test_loader:\n",
    "        \n",
    "        x = x.to(\"cuda:0\")\n",
    "        y = y.to(\"cuda:0\")\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = loss_new(output, y)\n",
    "        acc = accuracy(output, y)\n",
    "        \n",
    "        test_losses.append(loss.item())\n",
    "        test_acc.append(acc.item())\n",
    "\n",
    "    print(\"epoch loss\", sum(test_losses)/len(test_losses), \"acc\", sum(test_acc)/len(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
